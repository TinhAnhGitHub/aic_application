defaults:
  - hydra: default
  - _self_

hydra:
  searchpath:
    - file://conf/

mount_dir: /mnt/fangkai_blob
data_path_prefix: ${mount_dir}/share/
model_path_prefix: ${mount_dir}/share/models # ../pretrained-models/
output_path_prefix: ${mount_dir}/reward_modeling/

train_file:
dev_file:
test_file: ${data_path_prefix}/MWPBench/data/train_wo_gsm_2k.json

port: 6000
model:

sampling_params:
  _target_: vllm.SamplingParams
  n: 1
  temperature: 0.0
  max_tokens: 4096
  stop: [ "<eos>", "\n\n\n\n", "### Instruction", "<｜end▁of▁sentence｜>", "</s>", "<pad>" ]
  top_p: 0.9

tem: ${sampling_params.temperature}
n: ${sampling_params.n}
top_p: ${sampling_params.top_p}
split_size: 8
split_id: 0
max_num_seqs: 32

suffix: ${split_id}-of-${split_size}
#output_file: ${output_dir}/mathscale/${eval_sub_path}/train.v60.300k.2-of-30.v1.2.0shot.n${n}.tem${tem}.p${top_p}.${suffix}.json
output_file: ${output_dir}/mwpbench/${eval_sub_path}/train_wo_gsm.2k.v1.0.0shot.n${n}.tem${tem}.p${top_p}.${suffix}.json  # Align the implementation with Geyang: https://github.com/XingxingZhang/math_step/blob/iter_dev/dataset/pseudo/utils.py#L282-L328
flush_file: ${output_file}l

apply_chat_template: False
add_generation_prompt: True

read_tensor:
  _target_: data.combine_dataset.ResponseAlignDataset
#  aligner:
#    _target_: data.input_aligner.concat_aligner
#    aligners:
#      - _target_: data.mathscale.util.mathscale_extract_answer_fn_v3
#        completion_field: completion
##        kv_mapping:
##          instruction: question
#      - _target_: data.input_aligner.add_id_aligner
#        id_field: id
  template: "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{question}\n\n### Response:"
  instruction:
  split_size: ${split_size}
  split_id: ${split_id}
  service_based: False
  service_processor:
    _target_: data.vllm.VLLMRequestGenerator
    api_url: http://0.0.0.0:${port}/v1/completions
    max_tokens: ${sampling_params.max_tokens}
    model: ${model}
    stop: ${sampling_params.stop}
    n: ${n}
    temperature: ${tem}
    top_p: ${top_p}
  index_field: question_number


save_best: False
step:
exp_name:
exp_notes:
output_dir: ${output_path_prefix}/experiments/${exp_name}/
eval_sub_path: ""

# Dataloader
num_workers: 8
prefetch_factor: 2

dp_size:
tp_size: 1
pp_size: 1


post_process:
  _target_: post_processors.openai_api_callback.MathScaleCallBack
  answer_clean:
  output_file: ${output_file}
  resume: False
  index_field: "question_number"
  label_field: "answer"
  saved_keys: [ "question", "question_number", "data_source", "answer", "data_topic"]

# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
