defaults:
  - hydra: default
  - _self_

hydra:
  searchpath:
    - file://conf/

data_path_prefix: /mnt/fangkai_blob/share/
model_path_prefix: /mnt/fangkai_blob/share/models # ../pretrained-models/
output_path_prefix: /mnt/fangkai_blob/reward_modeling/

sample_over_p: 32
sft_model_dir: ${output_path_prefix}/experiments/mathstral.mathscale4o.process-dpo.iter0.V100.tp8dp48.v2.2.fix.s42/checkpoint-600/
#test_file_name: cot.de_con.n16.tem1.0.p1.0.split23.upper0.8.r0.3.sample8.filter_same.json
#test_file_name: cot.de_con.n16.tem1.0.p1.0.split01.upper0.8.r0.3.sample8.filter_same.json
#test_file_name: cot.de_con.n16.tem1.0.p1.0.split45.upper0.8.r0.3.sample8.filter_same.json
#test_file_name: cot.de_con.n16.tem1.0.p1.0.split67.upper0.8.r0.3.sample8.filter_same.json
#test_file_name: cot.de_con.n16.tem1.0.p1.0.split89.upper0.8.r0.3.sample8.filter_same.json
test_file_name: cot.de_con.n16.tem1.2.p1.0.split89.upper0.8.r0.3.sample32.filter_same.json
global_split_id: 0
train_file:
dev_file:
#test_file: ${data_path_prefix}/dataset/NuminaMath/numina-math-cot.de_con.label.${global_split_id}-of-10.json
#test_file: ${sft_model_dir}/numina/cot.de_con.n8.tem1.0.p1.0.s0.upper0.7.r0.3.sample${sample_over_p}.filter_same.${global_split_id}-of-4.json
#test_file: ${sft_model_dir}/numina/cot.de_con.n8.tem1.0.p1.0.orig_0-of-8.s0.upper0.7.r0.3.sample32.filter_same.json
test_file: ${sft_model_dir}/numina/${test_file_name}

port: 6000
model:

sampling_params:
  _target_: vllm.SamplingParams
  n: 3
  temperature: 1.0
  max_tokens: 2048
  stop: [ "<eos>", "\n\n\n\n", "### Instruction", "<｜end▁of▁sentence｜>", "</s>", "<pad>" ]
  top_p: 1.0

tem: ${sampling_params.temperature}
n: ${sampling_params.n}
top_p: ${sampling_params.top_p}
split_size: 1024
split_id: 0
max_num_seqs: 64
max_model_len: 4096
global_batch_size: 512

suffix: ${split_id}-of-${split_size}
#output_file: ${output_dir}/${eval_sub_path}/numina/830k-split-${global_split_id}-of-10/cot.de_con.${global_split_id}-of-10.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/completion-split-${split_size}/cot.de_con.n8.tem1.0.p1.0.s0.upper0.7.r0.3.sample${sample_over_p}.filter_same.${global_split_id}-of-4.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/completion-split-${split_size}/cot.de_con.n8.tem1.0.p1.0.orig_0-of-8.s0.upper0.7.r0.3.sample32.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.0.p1.0.split23.upper0.8.r0.3.sample8.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.0.p1.0.split01.upper0.8.r0.3.sample8.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.0.p1.0.split45.upper0.8.r0.3.sample8.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.0.p1.0.split67.upper0.8.r0.3.sample8.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
#output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.0.p1.0.split89.upper0.8.r0.3.sample8.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
output_file: ${output_dir}/${eval_sub_path}/numina/cot.de_con.n16.tem1.2.p1.0.split89.upper0.8.r0.3.sample32.filter_same.n${n}.tem${tem}.p${top_p}.${suffix}.s${seed}.json
flush_file: ${output_file}l

apply_chat_template: False
add_generation_prompt: True

read_tensor:
  _target_: data.combine_dataset.ResponseAlignDataset
  aligner:
    _target_: data.input_aligner.concat_aligner
    aligners:
      - _target_: data.input_aligner.flat_aligner
        input_index_field: id
        extract_field: [ "prefix", "prefix_id" ]
        mode: "multi"
  template: "{question}\n\nPlease put your final answer within {instruction}.{prefix}"
  instruction: "\\boxed{}"
  split_size: ${split_size}
  split_id: ${split_id}
  service_based: False
  service_processor:
    _target_: data.vllm.VLLMRequestGenerator
    api_url: http://0.0.0.0:${port}/v1/completions
    max_tokens: ${sampling_params.max_tokens}
    model: ${model}
    stop: ${sampling_params.stop}
    n: ${n}
    temperature: ${tem}
    top_p: ${top_p}
  index_field: prefix_id
  flush_file: ${flush_file}


save_best: False
step:
exp_name:
exp_notes:
#output_dir: ${model_path_prefix}/mathstral-7B-v0.1/
output_dir: ${sft_model_dir}
eval_sub_path: ""

# Dataloader
num_workers: 8
prefetch_factor: 2

dp_size:
tp_size: 1
pp_size: 1


post_process:
  _target_: post_processors.openai_api_callback.MathScaleCallBack
  answer_clean:
  output_file: ${output_file}
  resume: True
  index_field: "prefix_id"
  label_field: "label"
  saved_keys: [ "question", "id", "prefix", "prefix_id" ]

# Training hyper-parameters
per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1

ddp_eval: False
no_cuda: False
seed: 42
local_rank: -1

# Temporary variables
fp16: True
fp16_bfloat16: True
n_gpu: 1
device:
train_batch_size:
eval_batch_size:
world_size:
