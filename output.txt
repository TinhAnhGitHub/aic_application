===== app/common/repository/base.py =====
from __future__ import annotations
from typing import List, Optional, Sequence, Iterable, cast, Literal
from abc import abstractmethod

from pymilvus import AsyncMilvusClient, AnnSearchRequest, RRFRanker, WeightedRanker
from pymilvus import Function, FunctionType
from scipy.sparse import csr_matrix

from app.schemas.search_results  import MilvusSearchResponseItem



class MilvusVectorSearch:
    def __init__(
        self,
        uri: str,
        token: Optional[str],
        collection: str,
    ):
        self.client = AsyncMilvusClient(uri=uri, token=token)
        self.collection = collection

    async def close(self):
        await self.client.close()

    @staticmethod
    def _flatten_hits(search_result) -> Iterable:
        if isinstance(search_result, list) and search_result and isinstance(search_result[0], list):
            for hits in search_result:
                for hit in hits:
                    yield hit
        else:
            for hit in search_result:
                yield hit
    @staticmethod
    def _hit_to_item(hit) -> MilvusSearchResponseItem:
        return MilvusSearchResponseItem(
            identification=hit.entity.get("id"),
            score=hit.score,
        )

    def _to_items(self, search_result) -> List[MilvusSearchResponseItem]:
        return [self._hit_to_item(h) for h in self._flatten_hits(search_result)]

    @property
    @abstractmethod
    def dense_field(self) -> str: ...

    @property
    def sparse_field(self) -> Optional[str]:
        return None

    # ---- single dense search ----
    async def search_dense(
        self,
        query_embedding: list[float],
        top_k: int,
        param: dict,
        expr: Optional[str] = None,
        with_embedding: bool = False,
    ):
        ofs = ["id"]
        if with_embedding:
            ofs.append("embedding")

        res = await self.client.search(
            collection_name=self.collection,
            data=[query_embedding],
            anns_field=self.dense_field,
            param=param,
            limit=top_k,
            expr=expr,
            output_fields=ofs,
        )
        return self._to_items(res)

    async def construct_request_for(
        self,
        *,
        data: list[float] | csr_matrix,                 
        anns_field: str,      
        top_k: int,
        param: dict,
        expr: Optional[str] = None,
    ) -> AnnSearchRequest:

        
        return AnnSearchRequest(
            data=[data],
            anns_field=anns_field,
            param=param,
            limit=top_k,
            expr=expr,
        )
    async def search_combination(
        self,
        requests: list[AnnSearchRequest],
        output_fields: list[str],   
        rerank: Literal["rrf", "weighted"] = "rrf",
        weights: Optional[Sequence[float]] = None,
    ):
        if rerank == "weighted":
            weights = cast(Sequence[float], weights)
            assert len(requests) == len(weights), "Weights length must match requests"

        if rerank == "rrf":
            ranker = RRFRanker(k=60)
        else:
            weights = cast(Sequence[float], weights)
            ranker = WeightedRanker(*weights)

        ofs = output_fields or ["id"]

        res = await self.client.hybrid_search(
            collection_name=self.collection,
            reqs=requests,
            output_fields=ofs,            #
            ranker=ranker,
        )   
        return self._to_items(res)
    


===== app/common/repository/__init__.py =====
from base import MilvusVectorSearch

===== app/controller/search_controller.py =====
from typing import Dict, List, Optional, Tuple


from app.schemas.search_queries import SingleSearchRequest, TrakeSearchRequest, FusionMethod
from app.schemas.search_results import (
    KeyframeScore,
    ModalityResult,
    SingleSearchResponse,
    TrakePath,
    TrakePathResponse,
    FusionSummary,
    RRFDetail,
    WeightedDetail, 
    MilvusSearchResponseItem
)

from app.services.fusion_services import (
    rrf_fuse,
    weighted_fuse,
    organize_and_dedup_group_video_kf,
    normalize_event_scores_kf,
    beam_sequences_single_bucket_kf,
    rerank_across_videos_kf
)


from app.repository.elastic_repo import ElasticsearchKeyframeRepo  
from app.repository.keyframe_repo import KeyframeRepo
from app.services.tag_services import TagService
from app.services.search_services import SearchService
from app.schemas.search_settings import TopKReturn, ControllerParams
from app.services.model_services import ModelService



class SearchController:
    def __init__(
        self,
        ocr_repo: ElasticsearchKeyframeRepo,
        keyframe_repo: KeyframeRepo,
        search_service: SearchService,
        tag_service: TagService,
        model_service: ModelService
        
    ):
        self.ocr_repo = ocr_repo
        self.keyframe_repo = keyframe_repo
        self.search_service = search_service
        self.tag_service = tag_service
        self.model_service = model_service

    
    async def _milvus_to_keyframe_score(
        self,
        result: list[MilvusSearchResponseItem]
    ) -> list[KeyframeScore]:
        identifications = [str(r.identification) for r in result]
        keyframes = await self.keyframe_repo.get_many_by_identifications(identifications)
        id_to_kf = {kf.identification: kf for kf in keyframes}
        scores = []
        for r in result:
            kf = id_to_kf.get(r.identification)
            if kf:
                scores.append(
                    KeyframeScore(
                        identification=kf.identification,
                        group_id=kf.group_id,
                        video_id=kf.video_id,
                        keyframe_id=kf.keyframe_id,
                        tags=kf.tags,
                        ocr=kf.ocr,
                        score=r.score
                    )
                )
        return scores    

    async def _search_keyframe(self, text: str, topk: int, param: dict, tag_boost_alpha: float = 0.0) -> List[KeyframeScore]:
        assert self.model_service is not None and self.tag_service is not None
        emb = self.model_service.embed_text(text)
        milvus = await self.search_service.search_caption_dense(emb, topk, param)
        scored = await self._milvus_to_keyframe_score(milvus)
        if tag_boost_alpha > 0.0:
            tags = self.tag_service.scan_tags(text)
            scored = self.tag_service.rerank_keyframe_search_with_tags(tags, scored, tag_boost_alpha)
        return scored

    

    # async def _search_caption_dense(self, text: str, topk: int, param: dict, tag_boost_alpha: float = 0.0) -> List[KeyframeScore]:
    #     assert self.model_service is not None and self.tag_service is not None
    #     emb = self.model_service.embed_text(text)
    #     milvus = await self.search_service.search_caption_dense(emb, topk, param)
    #     scored = await self._milvus_to_keyframe_score(milvus)
    #     if tag_boost_alpha > 0.0:
    #         tags = self.tag_service.scan_tags(text)
    #         scored = self.tag_service.rerank_keyframe_search_with_tags(tags, scored, tag_boost_alpha)
    #     return scored

    async def _search_caption(
        self,
        text: str,
        topk: int,
        param:dict,
        tag_boost_alpha: float,
        fusion: FusionMethod,
        weighted: float | None 
    ):
        dense_emb = self.model_service.embed_text(text)
        dense_req = await self.search_service.caption_search.construct_dense_request(dense_emb, topk, param)

        milvus_hits = None
        try:
            sparse_vec = self.model_service.embed_sparse_text(text)
            sparse_req = await self.search_service.caption_search.construct_sparse_request(sparse_vec, topk, param)
            sparse_req = await self.search_service.caption_search.construct_sparse_request(sparse_vec, topk, param)
            if fusion == "weighted":
                w_dense = weighted if (weighted is not None) else 0.5
                w_sparse = 1.0 - w_dense
                milvus_hits = await self.search_service.caption_search.search_caption_hybrid(
                    dense_req=dense_req,
                    sparse_req=sparse_req,
                    rerank="weighted",
                    weights=[w_dense, w_sparse],
                )
            else:
                milvus_hits = await self.search_service.caption_search.search_caption_hybrid(
                    dense_req=dense_req,
                    sparse_req=sparse_req,
                    rerank="rrf",
                )
        except NotImplementedError:
            milvus_hits = await self.search_service.search_caption_dense(dense_emb, topk, param)

        scored = await self._milvus_to_keyframe_score(milvus_hits)
        if tag_boost_alpha > 0.0:
            tags = self.tag_service.scan_tags(text)
            scored = self.tag_service.rerank_keyframe_search_with_tags(tags, scored, tag_boost_alpha)
        return scored

    async def _search_ocr(self, text: str, topk: int) -> List[KeyframeScore]:
        assert self.ocr_repo is not None, "ocr_repo not set"
        return await self.ocr_repo.search(query_text=text, top_k=topk)





    async def single_search(self, req: SingleSearchRequest, topk: TopKReturn, ctrl: ControllerParams) -> SingleSearchResponse:
        per_modality: list[ModalityResult] = []
        lists_in_order: List[List[KeyframeScore]] = []

        if req.keyframe:
            kf = await self._search_keyframe(req.keyframe.text, topk.topk_visual, ctrl.kf_search_param, req.keyframe.tag_boost_alpha)
            per_modality.append(ModalityResult(modality="keyframe", items=kf))
            lists_in_order.append(kf)
        
        fusion_method = 'rrf'
        if req.caption:
            cap = await self._search_caption(
                text=req.caption.text,
                topk=topk.topk_caption,
                param=ctrl.cap_search_param,
                tag_boost_alpha=req.caption.tag_boost_alpha,
                fusion=req.caption.fusion,
                weighted=req.caption.weighted
            )
            lists_in_order.append(cap)
            fusion_method = req.caption.fusion
        
        if req.ocr:
            ocr = await self._search_ocr(req.ocr.text, topk.topk_ocr)
            per_modality.append(ModalityResult(modality="ocr", items=ocr))
            lists_in_order.append(ocr)

        if not any(lists_in_order):
            return SingleSearchResponse(fused=[], per_modality=[], fusion=FusionSummary(method="rrf", detail=RRFDetail(k=60)), meta={})

        if fusion_method == "weighted":
            wv, wc, wo = ctrl.fusion.ensure_scale()
            used_weights: List[float] = []
            for m in [pm.modality for pm in per_modality]:
                used_weights.append({"keyframe": wv, "caption": wc, "ocr": wo}[m])
            fused = weighted_fuse(lists_in_order, used_weights)
            fusion_summary = FusionSummary(method="weighted", detail=WeightedDetail(weights=used_weights, norm_score=True))
        else:
            fused = rrf_fuse(lists_in_order, k=60)
            fusion_summary = FusionSummary(method="rrf", detail=RRFDetail(k=60))

        fused = fused[:topk.final_topk]
        return SingleSearchResponse(fused=fused, per_modality=per_modality, fusion=fusion_summary, meta={})
    

    
    async def trake_search(
        self,
        req: TrakeSearchRequest,
        *,
        topk: TopKReturn,
        ctrl: ControllerParams,
        window: int = 6,
        beam_size: int = 50,
        per_bucket_top_k: Optional[int] = None,
        global_top_k: Optional[int] = 20,
        norm_method: str = "zscore",
        norm_temperature: float = 1.0,
        per_event_cap: Optional[int] = None
    ) -> tuple[TrakePathResponse, list[list[KeyframeScore]]]:
        """
        For each EventQuery (sorted by event_order):
          1) Run single_search for the event query.
          2) Build the event's candidate pool by UNION of per-modality lists (no cross-event fusion).
          3) Group by (group_id, video_id), de-dup per event (window).
          4) Normalize per event, beam search per bucket (temporal prior).
          5) Global rerank across buckets and return top paths.

        Return
            - The TrakePathResponse
            - The list[list[keyframe_score]] with raw score
        """

        events_sorted = sorted(req.events, key=lambda e: e.event_order)
        raw_hits_per_event: list[list[KeyframeScore]] = []

        for ev in events_sorted:
            single = await self.single_search(ev.query, topk, ctrl)
            fused_list: List[KeyframeScore] = single.fused
            if per_event_cap is not None and per_event_cap > 0:
                fused_list = fused_list[:per_event_cap]

            raw_hits_per_event.append(fused_list)
        
        by_group_video = organize_and_dedup_group_video_kf(raw_hits_per_event, window=window)

        by_bucket_paths: Dict[Tuple[str, str], List[Tuple[List[KeyframeScore], float]]] = {}
        for bucket, event_lists in by_group_video.items():
            norm_lists = normalize_event_scores_kf(
                event_lists,
                method=norm_method,
                temperature=norm_temperature,
            )
            paths = beam_sequences_single_bucket_kf(
                event_lists=norm_lists,
                beam_size=beam_size,
                K=per_bucket_top_k,
            )
            if paths:
                by_bucket_paths[bucket] = paths
        
        reranked = rerank_across_videos_kf(by_bucket_paths, top_k=global_top_k)
        trake_paths: list[TrakePath] = [
            TrakePath(items=path, score=score) for path, score in reranked
        ]
        return TrakePathResponse(paths=trake_paths, meta={}), raw_hits_per_event


===== app/models/common.py =====
from beanie import Document
from pydantic import BaseModel, Field
from datetime import datetime
from typing import Literal

from app.schemas.application import  KeyframeScore, CaptionSearch, KeyframeSearch, OCRSearch


class KeyframeModel(Document):
    identification: int = Field(..., description="A unique identifier for the keyframe.")
    group_id: str = Field(..., description="The group ID of the keyframe.")
    video_id: str = Field(..., description="The video ID associated with the keyframe.")
    keyframe_id: str = Field(..., description="The unique ID of the keyframe.")
    tags: list[str] | None = None

    class Settings:
        collection = "keyframes"
        indexes = [
            "group_id",
            "video_id",
            "keyframe_id",
            {"fields": ["identification"], "unique": True},
        ]



class ChatHistory(Document):
    """
    Represents a chat or question history item.
    """
    question_filename: str = Field(..., description="The name/identifier of the question or search.")
    timestamp: datetime = Field(default_factory=datetime.now, description="When the history item was created.")
    
    return_images: list[KeyframeScore] = Field(..., description="Search images associated with this history.")

    keyframe_search_text: KeyframeSearch | None = Field(None, description="The keyframe search text")
    caption_search_text: CaptionSearch | None = Field(None, description="The caption search text")
    ocr: OCRSearch | None = Field(None, description="List of OCR for matching")

    
    rerank: Literal['rrf', 'weigted'] | None = Field(None, description="Enable if both caption, keyframe search and OCR")
    weights: list[float] | None = Field(None, description="Weighted of visual embedding. Caption search will be (1-keyframe embedding)")
    


===== app/models/history.py =====
from beanie import Document, Indexed
from pydantic import BaseModel, Field
from typing import List, Literal, Optional
from datetime import datetime

from app.schemas.search_queries import SingleSearchRequest, TrakeSearchRequest
from app.schemas.search_results import SingleSearchResponse, TrakePathResponse

HistoryType = Literal["single", "trake"]


class HistoryEvent(BaseModel):
    event_order: int
    query: SingleSearchRequest

class HistoryResult(BaseModel):
    count: int
    top_idents: list[int]

class SearchHistory(Document):
    kind: HistoryType
    created_at: datetime 

    # Input
    single_request : SingleSearchRequest | None = None
    trake_request: TrakeSearchRequest | None = None

    #output
    single_response: SingleSearchResponse | None = None
    trake_response: TrakePathResponse | None = None

    # metadta
    tags_used: list[str] | None = None
    


===== app/repository/chat_repo.py =====
from typing import List, Optional, Union
from beanie import PydanticObjectId
from pymongo.results import DeleteResult, InsertManyResult
from app.models.common import ChatHistory


class ChatRepo:
    def __init__(self, model=ChatHistory):
        self.model = model

    # ---------- CREATE ----------
    async def create_one(self, item: Union[dict, ChatHistory]) -> ChatHistory:
        if isinstance(item, dict):
            item = self.model(**item)
        await item.insert()
        return item

    async def create_many(
        self, items: List[Union[dict, ChatHistory]]
    ) -> InsertManyResult:
        docs = [i if isinstance(i, self.model) else self.model(**i) for i in items]
        return await self.model.insert_many(docs)

    async def get_by_id(self, id_: PydanticObjectId) -> Optional[ChatHistory]:
        return await self.model.get(id_)

    async def get_by_question(
        self, question_filename: str, limit: int = 50
    ) -> List[ChatHistory]:
        return (
            await self.model.find(self.model.question_filename == question_filename)
            .sort(-self.model.timestamp)
            .limit(limit)
            .to_list()
        )

    async def list_all(self, limit: int = 100, skip: int = 0) -> List[ChatHistory]:
        return (
            await self.model.find_all()
            .sort(-self.model.timestamp)
            .skip(skip)
            .limit(limit)
            .to_list()
        )
    
    async def delete_by_id(self, id_: PydanticObjectId) -> Optional[DeleteResult]:
        doc = await self.model.get(id_)
        if doc:
            return await doc.delete()
        return None

    async def delete_by_question(self, question_filename: str) -> int:
        res = await self.model.find(self.model.question_filename == question_filename).delete()
        return res.deleted_count


===== app/repository/elastic_repo.py =====
from __future__ import annotations
from typing import Iterable, List, Optional, Sequence
from elasticsearch import AsyncElasticsearch, NotFoundError


from app.schemas.application import KeyframeInstance, KeyframeScore



class ElasticsearchKeyframeRepo:
    """
    Stores and searches KeyframeInstance by OCR using the Vietnamese analyzer.
    - ocr: vi_analyzer (with underthesea stopwords)
    - ocr.fold: ascii-folded analyzer for tone-less queries
    Search strategy ("mostly exact with a bit of fuzzy"):
      1) match_phrase on ocr (boost high)
      2) match_phrase on ocr.fold (boost high but a bit lower)
      3) match (AND) with fuzziness on both fields (low boost)
    """

    def __init__(
        self,
        hosts: Sequence[str] = ("http://localhost:9200",),
        index: str = "keyframes",
        api_key: Optional[str] = None,
        basic_auth: Optional[tuple[str, str]] = ("elastic", "changeme"),
        verify_certs: bool = False,
        request_timeout: int = 30,
    ):
        self.es = AsyncElasticsearch(
            hosts=hosts,
            api_key=api_key,
            basic_auth=basic_auth,
            verify_certs=verify_certs,
            request_timeout=request_timeout,
        )
        self.index = index
    
    def ensure_index(
        self,
        recreate: bool = False,
        stop_words: list[str] | None = None,
        keep_punctuation: Optional[bool] = None,
        dict_path: Optional[str] = None,
        split_url: Optional[bool] = None,
    ):
        if recreate:
            try:
                self.es.indices.delete(index=self.index)
            except NotFoundError:
                pass
                
        if self.es.indices.exists(index=self.index):
            return

        vi_params = {
            "stopwords": stop_words or [],
        }
        if keep_punctuation is not None:
            vi_params["keep_punctuation"] = keep_punctuation
        if dict_path is not None:
            vi_params["dict_path"] = dict_path
        if split_url is not None:
            vi_params["split_url"] = split_url
        
        settings = {
            "analysis": {
                "analyzer": {
                    "my_vi_custom": {"type": "vi_analyzer", **{k: v for k, v in vi_params.items() if v is not None}},
                    "my_vi_fold": {
                        "tokenizer": "vi_tokenizer",
                        "filter": ["lowercase", "my_ascii_folding"],
                    },
                },
                "filter": {
                    "my_ascii_folding": {"type": "asciifolding", "preserve_original": False}
                },
            }
        }

        body = {
            "settings": settings,
            "mappings": {
                "dynamic": "strict",
                "properties": {
                    "group_id": {"type": "keyword"},
                    "video_id": {"type": "keyword"},
                    "keyframe_id": {"type": "keyword"},
                    "identification": {"type": "integer"},
                    "tags": {"type": "keyword"},
                    # Store OCR list in _source, but index joined text for analysis
                    "ocr": {
                        "type": "text",
                        "analyzer": "my_vi_custom",
                        "search_analyzer": "my_vi_custom",
                        "fields": {
                            "fold": {"type": "text", "analyzer": "my_vi_fold"},
                        },
                    },
                    "_ocr_joined": {  # internal convenience (not queried directly here)
                        "type": "text",
                        "index": False,
                    },
                }
            },
        }

        self.es.indices.create(index=self.index, body=body)

    @staticmethod
    def _make_id(d: KeyframeInstance)->str:
        return f"{d.group_id}::{d.video_id}::{d.keyframe_id}"
        
    def upsert(self, item: KeyframeInstance):
        _id = self._make_id(item)
        body = item.model_dump(mode="json")
        if item.ocr:
            body["_ocr_joined"] = " ".join(item.ocr)
            body['ocr'] = [" ".join(item.ocr)]
        self.es.index(index=self.index, id=_id, document=body)

    def bulk_upsert(self, docs: Iterable[KeyframeInstance], refresh: bool = True):
        from elasticsearch.helpers import streaming_bulk

        def gen_actions():
            for d in docs:
                body = d.model_dump()
                if d.ocr:
                    body["_ocr_joined"] = " ".join(d.ocr)
                    body["ocr"] = [" ".join(d.ocr)]
                yield {
                    "_op_type": "index",
                    "_index": self.index,
                    "_id": self._make_id(d),
                    "_source": body,
                }

        for ok, _ in streaming_bulk(self.es, gen_actions()):
            if not ok:
                pass
        if refresh:
            self.es.indices.refresh(index=self.index)
    
    def get(self, group_id: str, video_id: str, keyframe_id: str) -> Optional[KeyframeInstance]:
        _id = f"{group_id}::{video_id}::{keyframe_id}"
        try:
            resp = self.es.get(index=self.index, id=_id)
        except NotFoundError:
            return None
        return KeyframeInstance(**resp["_source"])

    async def search(
        self,
        query_text: str,
        top_k: int = 10,
        group_id: Optional[str] = None,
        video_id: Optional[str] = None,
        min_score: Optional[float] = None,
        from_: int = 0,
        explain: bool = False,
    ) -> List[KeyframeScore]:
        """
        “Mostly exact” OCR search with a little fuzziness.

        Scoring order (highest -> lowest):
          A) Exact phrase match on ocr            (boost 6.0)
          B) Exact phrase match on ocr.fold       (boost 5.0)
          C) ANDed match with fuzziness on ocr    (boost 1.7)
          D) ANDed match with fuzziness on ocr.fold (boost 1.4)
        """

        filters: list[dict] = []
        if group_id is not None:
            filters.append({"term": {"group_id": group_id}})
        if video_id is not None:
            filters.append({"term": {"video_id": video_id}})
        
        shoulds = [
            {"match_phrase": {"ocr": {"query": query_text, "slop": 0, "boost": 6.0}}},
             {"match_phrase": {"ocr.fold": {"query": query_text, "slop": 0, "boost": 5.0}}},
             {
                "match": {
                    "ocr": {
                        "query": query_text,
                        "operator": "and",
                        "fuzziness": "AUTO:4,6",
                        "prefix_length": 1,
                        "max_expansions": 20,
                        "boost": 1.7,
                    }
                }
            },
            {
                "match": {
                    "ocr.fold": {
                        "query": query_text,
                        "operator": "and",
                        "fuzziness": "AUTO:4,6",
                        "prefix_length": 1,
                        "max_expansions": 20,
                        "boost": 1.4,
                    }
                }
            },
        ]

        body = {
            "query": {
                "bool": {
                    "should": shoulds,
                    "minimum_should_match": 1,
                    "filter": filters or None,
                }
            },
            "_source": True,
            "size": top_k,
            "from": from_,
        }

        if min_score is not None:
            body["min_score"] = min_score
        
        resp = await self.es.search(index=self.index, body=body, explain=explain)
        hits = resp.get('hits', {}).get('hits', [])
        out = []
        for hit in hits:
            src = hit.get("_source", {})
            out.append(
                KeyframeScore(
                    score=hit.get("_score", 0.0),
                    group_id=src['group_id'],
                    video_id=src['video_id'],
                    keyframe_id=src['keyframe_id'],
                    identification=src['identification'],
                    tags=src.get("tags"),
                    ocr=src.get("ocr")
                )
            )
        return out





===== app/repository/keyframe_repo.py =====
from __future__ import annotations

from typing import Iterable, List, Optional, Sequence, Union

from beanie import init_beanie, PydanticObjectId
from motor.motor_asyncio import AsyncIOMotorClient
from pydantic import BaseModel

from pymongo import ReplaceOne
from pymongo.results import BulkWriteResult, InsertManyResult, DeleteResult


from app.models.common import KeyframeModel


async def init_mongo(db_uri: str, db_name: str):
    """
    Call this once at startup.
    """
    client = AsyncIOMotorClient(db_uri)
    db = client[db_name]
    await init_beanie(database=db, document_models=[KeyframeModel])
    return client

class KeyframeRepo:
    def __init__(self, model=KeyframeModel):
        self.model = model

    async def create_one(
        self, item: Union[KeyframeModel, dict]
    ) -> KeyframeModel:
        if isinstance(item, dict):
            item = self.model(**item)
        await item.insert()  # sets item.id
        return item

    async def create_many(
        self, items: Sequence[Union[KeyframeModel, dict]], ordered: bool = False
    ) -> InsertManyResult:
        docs: List[KeyframeModel] = [
            i if isinstance(i, KeyframeModel) else self.model(**i) for i in items
        ]
        return await self.model.insert_many(docs, ordered=ordered)

    
    
    async def get_by_triplet(
        self, group_id: str, video_id: str, keyframe_id: str
    ) -> Optional[KeyframeModel]:
        return await self.model.find_one(
            (self.model.group_id == group_id)
            & (self.model.video_id == video_id)
            & (self.model.keyframe_id == keyframe_id)
        )

    async def get_many_by_identifications(
        self, identifications: Iterable[str]
    ) -> List[KeyframeModel]:
        id_list = list(identifications)

        docs = await self.model.find(
            self.model.identification.in_(list(identifications)) # type: ignore[attr-defined]
        ).to_list()

        lookup = {
            d.identification:d for d in docs
        }    
        ordered_docs = [lookup[i] for i in id_list if i in lookup]
        return ordered_docs


===== app/repository/vector_repo.py =====

from pymilvus import AnnSearchRequest
from typing import Literal, Optional, Sequence
from app.common.repository import MilvusVectorSearch
from scipy.sparse import csr_matrix



from app.schemas.search_results  import MilvusSearchResponseItem


class KeyframeSearchRepo(MilvusVectorSearch):
    KF_DENSE_FIELD = "kf_embedding"

    @property
    def dense_field(self) -> str:
        return self.KF_DENSE_FIELD
    


class CaptionSearchRepo(MilvusVectorSearch):
    CAPTION_DENSE_FIELD = "caption_embedding"
    CAPTION_SPARSE_FIELD = "caption_sparse"

    @property
    def dense_field(self) -> str:
        return self.CAPTION_DENSE_FIELD

    @property
    def sparse_field(self) -> str:
        return self.CAPTION_SPARSE_FIELD

    async def search_caption_dense(self, query_embedding: list[float], top_k: int, param: dict, **kwargs):
        return await self.search_dense(query_embedding, top_k, param, **kwargs)

    async def construct_dense_request(self, embedding: list[float], top_k: int, param: dict) -> AnnSearchRequest:
        return await self.construct_request_for(
            data=embedding,
            anns_field=self.dense_field,
            top_k=top_k,
            param=param,
        )

    async def construct_sparse_request(self, sparse_vec: csr_matrix, top_k: int, param: dict) -> AnnSearchRequest:
        return await self.construct_request_for(
            data=sparse_vec,            
            anns_field=self.sparse_field,
            top_k=top_k,
            param=param,
        )
    
    async def search_caption_hybrid(
        self,
        dense_req: AnnSearchRequest,
        sparse_req: AnnSearchRequest,
        rerank: Literal["rrf", "weighted"] = "rrf",
        weights: Optional[Sequence[float]] = None,
    ) -> list[MilvusSearchResponseItem]:
        return await self.search_combination(
            requests=[dense_req, sparse_req],
            rerank=rerank,
            weights=weights,
            output_fields=["id"],  
        )






===== app/schemas/application.py =====
# from pydantic import BaseModel, Field
# from typing import Literal
# from typing_extensions import Annotated, Union  


# class KeyframeInstance(BaseModel):
#     group_id: str
#     video_id: str
#     keyframe_id: str
#     identification: int = Field(..., description="The identification of the keyframe, corresponding to the index of the embeddings in the Milvus Collection")
#     tags: list[str] | None = Field(None, description="List of tags associated with the keyframe")
#     ocr: list[str] | None = Field(None, description="List of OCR texts associated with the keyframe")


# class KeyframeScore(KeyframeInstance):
#     score: float

# class MilvusSearchRequestInput(BaseModel):
#     """
#     Input schema for Milvus vector search requests.
#     """

#     embedding: list[float] = Field(..., description="The embedding vector to search for.")
#     top_k: int = Field(..., description="The number of top similar items to retrieve.")


# class MilvusSearchResponseItem(BaseModel):
#     """
#     Response item schema for Milvus vector search results.
#     """
#     identification: int = Field(..., description="The identification of the keyframe, corresponding to the index of the embeddings in the Milvus Collection")
#     score: float = Field(..., description="The similarity score of the retrieved item.")
    


# class TagInstance(BaseModel):
#     tag_name: str
#     tag_score: float
# class CaptionSearch(BaseModel):
#     type_search: str = 'caption_search'
#     caption_search_text: str = Field(..., description="The keyframe search text")
#     mode: Literal['rrf', 'weighted']
#     weighted: float | None = Field(None, description="The weighted if using weighted, of the embedding. The bm25 will be (1 - embedding_weight)")
#     tag_boost_alpha: float = Field(...,ge=0, le=1.0, description="Tag boost alpha, if 0.0 then it will not be used")

# class KeyframeSearch(BaseModel):
#     type_search: str = 'keyframe_search'
#     keyframe_search_text: str = Field(..., description="The keyframe search text")
#     tag_boost_alpha: float = Field(...,ge=0, le=1.0, description="Tag boost alpha, if 0.0 then it will not be used")

# class OCRSearch(BaseModel):
#     type_search: str = 'ocr_search'
#     list_ocr: str = Field(..., description="List of OCR")




# # class EventOrder(BaseModel):
# #     """
# #     Event Text, chunked from the original text, with order to indicate the sequence.
# #     """
# #     order: int 
# #     event_text: str

# class EventSearch(BaseModel):
#     keyframe_search: KeyframeSearch | None = Field(None , description="Keyframe search")
#     caption_search: CaptionSearch | None = Field(None, description="Caption search")
#     ocr_search: OCRSearch | None = Field(None, description="OCR search")
#     event_order: int = Field(..., description="The event order")
    



# class EventHit(BaseModel):
#     search_setting: EventSearch = Field(..., description="Search setting")
#     video_id: str
#     keyframe_id: str
#     group_id: str
#     score: float




===== app/schemas/search_queries.py =====
from pydantic import BaseModel, Field
from typing import Literal, Optional, List, Union

SearchModality = Literal['keyframe', 'caption', 'ocr']
FusionMethod = Literal['rrf', 'weighted']

class BaseModalityQuery(BaseModel):
    modality: SearchModality
    tag_boost_alpha: float = Field(0.0, ge=0.0, le=1.0)

class KeyframeQuery(BaseModalityQuery):
    modality: Literal['keyframe'] = 'keyframe'
    text: str  

class CaptionQuery(BaseModalityQuery):
    modality: Literal['caption'] = 'caption'
    text: str
    fusion: FusionMethod = 'rrf'
    weighted: float | None = Field(None, description="If 'weighted': weight for dense; (1-weight) for sparse")


class OCRQuery(BaseModalityQuery):
    modality: Literal['ocr'] = 'ocr'
    text: str


class SingleSearchRequest(BaseModel):
    keyframe: Optional[KeyframeQuery] = None
    caption: Optional[CaptionQuery] = None
    ocr: Optional[OCRQuery] = None

class EventQuery(BaseModel):
    event_order: int
    query: SingleSearchRequest

class TrakeSearchRequest(BaseModel):
    events: list[EventQuery]




===== app/schemas/search_results.py =====
# app/schemas/search_results.py
from pydantic import BaseModel, Field
from typing import List, Optional, Union
from app.schemas.search_queries import SearchModality, FusionMethod




class MilvusSearchResponseItem(BaseModel):
    """
    Response item schema for Milvus vector search results.
    """
    identification: int = Field(..., description="The identification of the keyframe, corresponding to the index of the embeddings in the Milvus Collection")
    score: float = Field(..., description="The similarity score of the retrieved item.")


class RRFDetail(BaseModel):
    k: int = 60

class WeightedDetail(BaseModel):
    weights: List[float]  
    norm_score: bool = True

class KeyframeScore(BaseModel):
    identification: int
    group_id: str
    video_id: str
    keyframe_id: str
    tags: Optional[list[str]] = None
    ocr: Optional[list[str]] = None
    score: float

class ModalityResult(BaseModel):
    modality: SearchModality
    items: List[KeyframeScore]

class FusionSummary(BaseModel):
    method: FusionMethod
    detail: Union[RRFDetail, WeightedDetail, None] = None

class SingleSearchResponse(BaseModel):
    fused: list[KeyframeScore]
    per_modality: list[ModalityResult]
    fusion: FusionSummary
    meta: dict = Field(default_factory=dict)

class TrakePath(BaseModel):
    items: List[KeyframeScore]  # One keyframe for each event. 
    score: float

class TrakePathResponse(BaseModel):
    paths: List[TrakePath]   
    meta: dict = Field(default_factory=dict)

===== app/schemas/search_settings.py =====
from pydantic import BaseModel, Field
import numpy as np


class FusionWeights:
    w_visual: float = Field(default=0.5)
    w_caption: float = Field(default=0.3)
    w_ocr: float = Field(default=0.2)

    def ensure_scale(self) -> tuple[float,float,float]:
        """
        scale make sure total = 1
        """
        weights = np.array([
            self.w_visual, self.w_caption, self.w_ocr
        ], dtype=float)

        min_val, max_val = weights.min(), weights.max()

        if min_val == max_val:
            normed = np.ones_like(weights) / len(weights)
        
        else:
            normed = (weights - min_val) / (max_val - min_val)
        
        return normed[0], normed[1], normed[2]





class TopKReturn(BaseModel):
    topk_visual: int = Field(default=200)
    topk_caption: int = Field(default=200)
    topk_ocr: int = Field(default=400)
    final_topk: int = Field(default=300)


class ControllerParams(BaseModel):
    fusion: FusionWeights = Field(default_factory=FusionWeights)
    topk_settings: TopKReturn = Field(default_factory=TopKReturn)
    kf_search_param: dict = Field(default_factory=lambda: {"metric_type": "IP", "params": {"nprobe": 16}})
    cap_search_param: dict = Field(default_factory=lambda: {"metric_type": "IP", "params": {"nprobe": 16}})

    



===== app/services/fusion_services.py =====
from typing import Dict, Iterable, List, Optional, Sequence, Tuple
from statistics import pstdev, mean

from app.schemas.search_results import KeyframeScore

def rrf_fuse(
    per_lists:list[list[KeyframeScore]],
    k: int = 60
):
    rank_maps: list[dict[int,int]] = []
    rep: dict[int, KeyframeScore] = {}

    for items in per_lists:
        ordered = sorted(items, key=lambda x: x.score, reverse=True)
        rank_maps.append(
            {it.identification: idx + 1 for idx, it in enumerate(ordered)}
        )
        for it in items:
            if it.identification not in rep:
                rep[it.identification] = it
    
    all_ids = set().union(*(set(m) for m in rank_maps)) if rank_maps else set()
    fused: List[KeyframeScore] = []

    for ident in all_ids:
        s = 0.0
        for m in rank_maps:
            if ident in m:
                s += 1.0 / (k + m[ident])
        r = rep[ident]
        fused.append(r.__class__(**{**r.model_dump(), "score": float(s)}))
    fused.sort(key=lambda x: x.score, reverse=True)
    return fused


def weighted_fuse(
    per_lists: list[list[KeyframeScore]],
    weights: Sequence[float],
) -> list[KeyframeScore]:
    
    assert len(per_lists) == len(weights), "weights length must match number of lists"
    z_maps: List[Dict[int, float]] = []
    rep: Dict[int, KeyframeScore] = {}

    for items in per_lists:
        for it in items:
            rep.setdefault(it.identification, it)
    
    for items in per_lists:
        if not items:
            z_maps.append({})
            continue
        scores = [it.score for it in items]
        mu = mean(scores)
        sd_raw = pstdev(scores) if len(scores) > 1 else 0.0
        sd = sd_raw if sd_raw > 1e-6 else 1.0
        z_maps.append({it.identification: (it.score - mu) / sd for it in items})
    
    all_ids = set().union(*(set(m) for m in z_maps)) if z_maps else set()
    out: List[KeyframeScore] = []
    for ident in all_ids:
        s = 0.0
        for w, m in zip(weights, z_maps):
            if ident in m:
                s += float(w) * m[ident]
        r = rep[ident]
        out.append(r.__class__(**{**r.model_dump(), "score": float(s)}))

    out.sort(key=lambda x: x.score, reverse=True)
    return out




def _kf_pos(h: KeyframeScore)->int:
    return int(h.keyframe_id)


def _dedup_hits_kf(
    hits: list[KeyframeScore], window: int = 6
)-> list[KeyframeScore]:
    if not hits: 
        return []

    hits = sorted(hits, key=lambda h: _kf_pos(h))
    kept: list[KeyframeScore] = []
    i, n = 0, len(hits)

    while i < n:
        start = _kf_pos(hits[i])
        j = i
        segment: list[KeyframeScore] = []
        while j < n and _kf_pos(hits[j]) < start + window:
            segment.append(hits[j])
            j += 1

        best = max(segment, key=lambda x: x.score)
        kept.append(best)
        while j < n and (_kf_pos(hits[j]) - start) < window:
            j += 1
        i = j
    return kept


def organize_and_dedup_group_video_kf(
    hits_per_event: List[List[KeyframeScore]],
    window: int = 6,
) -> Dict[Tuple[str, str], List[List[KeyframeScore]]]:
    if not hits_per_event:
        return {}
    T = len(hits_per_event)

    tmp: Dict[Tuple[str, str], Dict[int, List[KeyframeScore]]] = {}
    for event_index, event_hits in enumerate(hits_per_event):
        for h in event_hits:
            key = (h.group_id, h.video_id)
            tmp.setdefault(key, {}).setdefault(event_index, []).append(h)

    by_group_video: Dict[Tuple[str, str], List[List[KeyframeScore]]] = {}
    for bucket, per_event in tmp.items():
        dedup_lists: List[List[KeyframeScore]] = []
        completed = True
        for e_idx in range(T):
            ev_list = per_event.get(e_idx, [])
            ev_list = _dedup_hits_kf(ev_list, window)
            if not ev_list:
                completed = False
                break
            ev_list.sort(key=lambda x: x.score, reverse=True)
            dedup_lists.append(ev_list)
        if completed:
            by_group_video[bucket] = dedup_lists
    return by_group_video

def _clone_with_score_kf(h: KeyframeScore, new_score: float) -> KeyframeScore:
    return h.__class__(**{**h.model_dump(), "score": float(new_score)})



def normalize_event_scores_kf(
    event_lists: List[List[KeyframeScore]],
    method: str = "zscore",  # "zscore" | "minmax"
    eps: float = 1e-6,
    temperature: float = 1.0,
) -> List[List[KeyframeScore]]:
    """
    Normalize scores within each event list (per bucket).
    """
    norm_lists: List[List[KeyframeScore]] = []
    for ev_hits in event_lists:
        scores = [h.score for h in ev_hits]
        if not scores:
            norm_lists.append([])
            continue
        if method == "zscore":
            mu = mean(scores)
            sd = pstdev(scores) if len(scores) > 1 else 0.0
            sd = sd if sd > eps else 1.0
            normed = [(s - mu) / sd for s in scores]
        else:
            lo, hi = min(scores), max(scores)
            rng = hi - lo
            rng = rng if rng > eps else 1.0
            normed = [(s - lo) / rng for s in scores]
        if temperature and temperature != 1.0:
            t = float(temperature)
            normed = [s / t for s in normed]
        norm_hits = [_clone_with_score_kf(h, s) for h, s in zip(ev_hits, normed)]
        norm_hits.sort(key=lambda x: x.score, reverse=True)
        norm_lists.append(norm_hits)
    return norm_lists



def beam_sequences_single_bucket_kf(
    event_lists: List[List[KeyframeScore]],   # one bucket: [E0 list, E1 list, ...], all non-empty
    K: Optional[int] = 5,
    beam_size: int = 50,
    trans_sigma: float = 1.5 * 6,
    trans_weight: float = 0.6,
) -> List[Tuple[List[KeyframeScore], float]]:
    """
    Beam search over ordered events for a single (group_id, video_id) bucket.
    Enforces strictly increasing keyframe_id and adds Gaussian temporal prior.
    """
    import heapq

    def temporal_prior(prev: KeyframeScore, curr: KeyframeScore) -> float:
        gap = _kf_pos(curr) - _kf_pos(prev)
        return - (gap * gap) / (2 * trans_sigma * trans_sigma) * trans_weight

    first = event_lists[0]
    beam: List[Tuple[float, List[KeyframeScore]]] = [(-h.score, [h]) for h in first]
    heapq.heapify(beam)
    beam = heapq.nsmallest(beam_size, beam)

    for idx in range(1, len(event_lists)):
        nxt: List[Tuple[float, List[KeyframeScore]]] = []
        for neg, path in beam:
            prev = path[-1]
            base = -neg
            for cur in event_lists[idx]:
                if _kf_pos(cur) <= _kf_pos(prev):
                    continue
                new_score = base + cur.score + temporal_prior(prev, cur)
                heapq.heappush(nxt, (-new_score, path + [cur]))
        if not nxt:
            return []
        beam = heapq.nsmallest(beam_size, nxt)

    if K is None:
        return [(path, -neg) for (neg, path) in beam]
    topK = min(K, len(beam))
    best = heapq.nsmallest(topK, beam)
    return [(path, -neg) for (neg, path) in best]



def rerank_across_videos_kf(
    by_bucket_paths: Dict[Tuple[str, str], List[Tuple[List[KeyframeScore], float]]],
    top_k: Optional[int] = None
) -> List[Tuple[List[KeyframeScore], float]]:
    flat: List[Tuple[List[KeyframeScore], float]] = []
    for _, paths in by_bucket_paths.items():
        flat.extend(paths)
    flat.sort(key=lambda x: x[1], reverse=True)
    return flat if top_k is None else flat[:top_k]

===== app/services/model_services.py =====
from app.services.unilm.beit3 import modeling_finetune
from torchvision import transforms
from transformers import XLMRobertaTokenizer
from timm import create_model
from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
from scipy.sparse import csr_matrix
from app.services.sparse_encoder import MilvusSparseEncoder


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TORCH_DTYPE = torch.float32


class Processor():
    def __init__(self, tokenizer):
        self.image_processor = transforms.Compose([
            transforms.Resize((384, 384), interpolation=3),
            transforms.ToTensor(),
            transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)
        ])
        
        self.tokenizer = tokenizer
    
    def process(self, image=None, text=None):
        assert (image is not None) or (text is not None)
        language_tokens = None
        padding_mask = None
        if image is not None:
            image = self.image_processor(image)
            image = image.unsqueeze(0)
        if text is not None:
            language_tokens, padding_mask, _ = self.get_text_segment(text)
        return {'image': image, 'text_description': language_tokens, 'padding_mask': padding_mask}
            
        
    def get_text_segment(self, text, max_len=64):
        tokens = self.tokenizer.tokenize(text)
        tokens = self.tokenizer.convert_tokens_to_ids(tokens)

        if len(tokens) > max_len - 2:
            tokens = tokens[:max_len - 2]

        tokens = [self.tokenizer.bos_token_id] + tokens[:] + [self.tokenizer.eos_token_id]
        num_tokens = len(tokens)
        padding_mask = [0] * num_tokens + [1] * (max_len - num_tokens)
        language_tokens = tokens + [self.tokenizer.pad_token_id] * (max_len - num_tokens)
        return torch.tensor([language_tokens]),  torch.tensor([padding_mask]), num_tokens
    
    def process_batch(self, images):
        batch_images = [self.image_processor(img) for img in images]
        batch_images = torch.stack(batch_images, dim=0)  
        return  batch_images
    


class ModelService:
    """Singleton service for BEiT3 (image) + SentenceTransformer (text) embeddings"""

    def __init__(
        self,
        beit3_ckpt: str,
        beit3_tokenizer_path: str,
        text_model_name: str = "AITeamVN/Vietnamese_Embedding",
        sparse_encoder: MilvusSparseEncoder | None = None
    ):
        self.device = DEVICE
        self.sparse_encoder = sparse_encoder
        self._init_text(text_model_name)
        self._init_vision(beit3_ckpt, beit3_tokenizer_path)

    def _init_text(self, model_name: str):
        model = SentenceTransformer(model_name, device=self.device)
        model.max_seq_length = 2048
        model.eval()
        self.text_model = model

    def _init_vision(self, ckpt_path: str, tokenizer_path: str):
        self.vision_model = create_model("beit3_large_patch16_384_retrieval")
        ckpt = torch.load(ckpt_path, map_location="cpu")
        self.vision_model.load_state_dict(ckpt["model"], strict=True)
        self.vision_model.to(self.device, dtype=TORCH_DTYPE).eval()

        self.tokenizer = XLMRobertaTokenizer(tokenizer_path)
        self.processor = Processor(self.tokenizer)
    
    @torch.inference_mode()
    def embed_text(self, text: str) -> list[float]:
        embs = self.text_model.encode(
            [text],
            batch_size=1,
            convert_to_tensor=True,
            show_progress_bar=False,
            device=self.device,
        )
        embs = torch.nn.functional.normalize(embs, p=2, dim=-1)
        return embs[0].detach().cpu().numpy().astype(np.float32).tolist()
    

    def embed_sparse_text(self, text: str) -> csr_matrix:
        if not self.sparse_encoder:
            raise NotImplementedError("Sparse encoder not configured")
        return self.sparse_encoder.encode(text)



===== app/services/search_services.py =====
from app.repository.vector_repo import KeyframeSearchRepo, CaptionSearchRepo
from typing import Literal, Optional, Sequence
class SearchService:
    def __init__(
        self,
        keyframe_search: KeyframeSearchRepo,
        caption_search: CaptionSearchRepo,
    ):
        self.keyframe_search = keyframe_search
        self.caption_search = caption_search

    
    async def search_keyframe_dense(self, query_embedding: list[float], top_k: int, param: dict, **kwargs):
        return await self.keyframe_search.search_dense(query_embedding, top_k, param, **kwargs)
    
    async def search_caption_dense(self, query_embedding: list[float], top_k: int, param: dict, **kwargs):
        return await self.caption_search.search_caption_dense(query_embedding, top_k, param, **kwargs)

    
    




    

===== app/services/sparse_encoder.py =====
from typing import  Dict, List, Optional
import math
from underthesea import word_tokenize
from scipy.sparse import csr_matrix



class MilvusSparseEncoder:
    def __init__(
        self,
        vocab: Dict[str, int],
        idf_by_tid: Optional[Dict[int, float]] = None,
        vocab_size: Optional[int] = None,
        lowercase: bool = True,
        l2_normalize: bool = True,
    ):
        self.vocab = vocab
        self.idf_by_tid = idf_by_tid or {}
        self.lowercase = lowercase
        self.l2_normalize = l2_normalize
        self.vocab_size = vocab_size
    
    def _tokenize(self, text: str) -> List[str]:
        return word_tokenize(text, format="text").split()

    def encode(self, text: str) ->csr_matrix:
        tokens = self._tokenize(text)
        if not tokens:
            return csr_matrix((1, self.vocab_size))

        tf: dict[int,int] = {}
        for tok in tokens:
            tid = self.vocab.get(tok)
            if tid is None:
                continue
            tf[tid] = tf.get(tid, 0) + 1

        if not tf: 
            return csr_matrix((1, self.vocab_size))

        indices: List[int] = []
        values: List[float] = []
        for tid, cnt in tf.items():
            idf = self.idf_by_tid.get(tid, 1.0)
            w = (1.0 + math.log(cnt)) * idf
            indices.append(tid)
            values.append(float(w))

        if self.l2_normalize and values:
            norm = math.sqrt(sum(v * v for v in values)) or 1.0
            values = [v / norm for v in values]

        return csr_matrix(
            (values, ([0] * len(indices), indices)),
            shape=(1, self.vocab_size),
        )

===== app/services/tag_services.py =====
from __future__ import annotations
import numpy as np
import regex as re
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from rapidfuzz import fuzz 
from underthesea import word_tokenize

from app.schemas.application import TagInstance
from app.schemas.application import KeyframeScore



def vn_tokenizer(text: str) -> list[str]:
    return word_tokenize(text, format="text").split()


class TagService:
    def __init__(
        self, 
        tag_list: list[str],
        preselect_k: int = 100,
        bm25_weight: float = 0.65,
        tfidf_weight: float = 0.30,
        rf_weight: float = 0.05
    ):
        if not tag_list:
            raise ValueError("tag_list must not be empty")
        
        self.bm25_weight = bm25_weight / (bm25_weight + tfidf_weight + rf_weight)
        self.tfidf_weight = tfidf_weight / (bm25_weight + tfidf_weight + rf_weight)
        self.rf_weight = rf_weight / (bm25_weight + tfidf_weight + rf_weight)


        self.tag_list = tag_list
        self._tags_np = np.array(tag_list)

        self._tfidf_word = TfidfVectorizer(
            tokenizer=vn_tokenizer,
            ngram_range=(1, 2),   # unigrams + bigrams of words
            lowercase=True,
            norm="l2"
        )
        self._X_word = self._tfidf_word.fit_transform(tag_list)


        self._tokenized_tags = [vn_tokenizer(tag) for tag in tag_list]
        self._bm25 = BM25Okapi(self._tokenized_tags)
        self._preselect_k = max(
            10, 
            min(preselect_k, len(tag_list))
        )

    def scan_tags(
        self,
        user_query: str,
        top_tags: int = 6
    ) -> list[TagInstance]:
        """
        From the user query, scan each tag against the query, and return the top 6 tags
        Using TfIDF, BM25 and rapid fuzz
        """
        if not user_query.strip():
            return []
        
        q_word = self._tfidf_word.transform([user_query])

        scored_words = (self._X_word @ q_word.T).toarray().ravel()

        M = min(
            self._preselect_k, len(self.tag_list)
        )

        # tdidf pre selection
        if M < len(self.tag_list):
            candidate_index = np.argpartition(-scored_words, M)[:M]
        else:
            candidate_index = np.arange(len(self.tag_list))
        
        # BM25 RANK
        q_tokens = vn_tokenizer(user_query)
        bm25_all = self._bm25.get_scores(q_tokens)
        bm25_scores = bm25_all[candidate_index]

        rf = np.array([
            fuzz.token_set_ratio(user_query, self.tag_list[i]) / 100.0 for i in candidate_index
        ])

        def norm(x: np.ndarray) -> np.ndarray:
            m = x.max() if x.size else 0.0
            return (x / m) if m > 0 else np.zeros_like(x)

        s_word = norm(scored_words[candidate_index])
        s_bm25 = norm(bm25_scores)
        s_rf = norm(rf) 

        final = self.bm25_weight * s_bm25 + self.tfidf_weight * s_word + self.rf_weight * s_rf

        k = min(top_tags, len(candidate_index))
        top_local = np.argpartition(-final, k - 1)[:k]
        top_sorted = top_local[np.argsort(-final[top_local])]

        results = []
        for j in top_sorted:
            idx = candidate_index[j]
            score = float(final[j])
            results.append(TagInstance(
                tag_name=self.tag_list[idx],
                tag_score=score
            ))
        return results
    

    def rerank_keyframe_search_with_tags(
        self,
        tags: list[TagInstance],
        results_search: list[KeyframeScore],
        alpha: float = 0.2
    ) -> list[KeyframeScore]:
        """Rerank the keyframe search results with the tags

        Args:
            tags (list[TagInstance]): List of tags with name and score
            results_search (list[MilvusSearchResponseItem]): list of returned keyframes
        """

        if not results_search:
            return []

        top_tag_names = {t.tag_name for t in tags}

        clip_scores = np.array([
            kf.score for kf in results_search
        ], dtype=float)

        mu = clip_scores.mean()
        sigma = clip_scores.std() if clip_scores.std() > 1e-6 else 1.0
        norm_scores = (clip_scores - mu) / sigma

        final_scores = []

        for i, kf in enumerate(results_search):
            if hasattr(kf, "tags") and kf.tags:
                kf_tags = set(kf.tags)
            else:
                kf_tags = set()
        

            m = len(kf_tags & top_tag_names)
            boost = alpha * np.log1p(m) * sigma
            final_score = norm_scores[i] + boost
            final_scores.append(final_score)
        
        for kf, fs in zip(results_search, final_scores):
            kf.score = float(fs)
        
        reranked = sorted(
            results_search,
            key=lambda x: x.score,
            reverse=True
        )

        return reranked



